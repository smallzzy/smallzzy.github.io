---
layout: post
title:
date: 2020-11-17 19:39
category:
author:
tags: []
summary:
---

## technology

- transistor
  - SRAM: hold value with power
  - register file: basicially memory for SRAM
    - decoder + transmission gate
- capacitor
  - DRAM: must be periodically refreshed
- floating gate
  - eeprom: byte programmable
  - flash: block (page) programmable

## DRAM

channel, dimm, rank, chip, bank, row, column

- memory controller: multiple mc per CPU
- channel: multiple channel per mc
  - one channel can support multiple dimm as rank
  - different channel should be driven independently
- dimm: physical stick
  - single inline = 32 bit ram data width
  - dual inline = 64 bit ram data width
- rank: different set of chips can be selected (1R, 2R)
  - max rank per channel is limited by CPU design
  - different slot in the same channel = combining dimm as rank
    - I assume there will be csrow swapping when designing motherboard
- chip: each chip only provide part of ram data width (x4, x8)
  - all chips are activated at the same time
  - 16 chips \* 4 bit per chip = 64 bit

Inside of each chip:

- bank: each bank output the same width as the chip
  - bank with same bank addr on different chip are activated at the same time
  - but row can be open in different banks independently
  - bank group: refreshing can be done on the same bank in different bank group
- row (pages): RAS
  - row address
- column: CAS
  - column address
  - After row is opened, next column can opened without re-open row
- cell: ex: 16384 rows x 1024 column x 4 cell
  - after address is given, W cells will generate the output. (W = chip data width)

## optimization

- prefetch: have wider connection at column output and cache the extra bits in burst buffer
  - so that we get the consecutive result without even going through cas
- sub-dividing row & column: so that it is easier to charge up & discharge
  - less capacitor load at row
  - closer to sense amplifier at column
- diff pair
- memory interleaving
  - memory interleaving allows consecutive virtual address to map to different dram device
    - n-way interleaving = # of dram device involved
  - interleaving is done on multiples of cache line size
    - cache line size = 64 bytes

https://youtu.be/7J7X7aZvMXQ

### address after interleaving

- CS-BG-Bank-CID-Row-Col: random access
- CS-CID-Row-Col-Bank-BG: multiple bank in one chip
- CID-Row-CS-Bank-Col-BG: multiple bank, multiple chip

https://www.mail-archive.com/linux-kernel@vger.kernel.org/msg2268475.html
https://lore.kernel.org/lkml/20210507190140.18854-1-Yazen.Ghannam@amd.com/

## Timing

Row Active -> Pre Charge -> Row Open -> Column Open

1. CAS Latency (`CL`): Cycle between sending a column address and begining of data in response
2. Row Address to Column Address Delay (`T_RCD`): Minimum cycle between opening a row and accessing columen within it
3. Row Precharge time (`T_RP`): Minimium cycle between pre charge to opening the next row
4. Row Active time (`T_RAS`): Minimium cycle between row active to pre charge

### more ram

UDIMM

https://www.intel.com/content/www/us/en/support/programmable/articles/000078029.html

Traditionally, memory chips are directly driven by controller.
Every chip will introduce some capacitive load to the driver.
The load become unacceptable when supporting more memory.

- Registered DIMM: control signal are hidden behind chip
- Load Reduced DIMM: both data and control signal are hidden behind buffer
  - allow
- Fully Buffered DIMM:
  - Put a driver chip between controller and memory
  - Use high frequency serial connection to this driver chip
    - let driver chip connect to more memory chips

LPDDR: more channel? fixed width per channel (16 bit)?

## bandwidth calculation

- data is transmitted based on Word clock (WCK)
- data rate: SDR, DDR, QDR
  - data can be toggled multiple times on one clock
    - implemented as mutliple phase shifted clock
  - memory clock = WCK \* data rate
- prefetch: 4n, 8n (these are advised prefetch)
  - burst read from one bank without CAS?
  - actual prefetch = advised prefetch / data rate ?
  - effective clock = memory clock \* actual prefetch
- bank grouping? QAM?
- controller can support wider memory interface
  - i.e. lane, pins
  - bandwidth = effective clock \* lane size
  channel * mt/s * 64bit?

http://monitorinsider.com/GDDR5X.html

```bash
# memory layout
ras-mc-ctl --mainboard?
ras-mc-ctl --layout
ras-mc-ctl --errors
ras-mc-ctl --summary
```

edac-ctl

numactl -H

dmidecode -t memory
lshw -class memory


## ecc

- side band ecc: ecc as additional data width
  - DIMM - 64 bit data width + 8 bit ecc width
  - add additional ram chip
- inline ecc: ecc as separate memory area
  - Controller will read / write to those areas separately
  - lpddr has a fixed channel width?
- on-die ecc: ecc is generated & fixed in die

## memory controller

### AMD infinity fabric

BCLK: base clock 100MHz
MCLK: memory clock
UCLK: memory controller clock
FCLK: fabric clock

- BCLK is traditionally used to generate other clock
  - CPU Ratio
  - Memory Multiplier
  - Async clock feature: allow CPU to use different BCLK for over clocking
- MCLK is half of advertised memory clock because of DDR
- MCLK vs UCLK
  - 1:1 is probably the only useful setting?
- UCLK vs FCLK
  - 1:1 -> coupled mode -> lower latency
  - diff -> uncoupled mode -> higher latency, possible high bandwidth

## cache replacement policy

victim cache: Located in further cache, capture recently evicted cache (victim)

## ecc check

dmidecode -t memory
Total Width > Data Width
total width might be wrong if bios ignored smbios?

MCA -> LMCA

### ecc mode

1. ECC - SECDED
2. Online Spare - Replace faulty region with spare region
   2. Post Package Repair
3. Mirror / Lockstep
   1. Memory transaction will be performed twice
4. SDDC: Single Device Data Correction
   1. If a single chip failed, it will lead to consecutive bit error. Which cannot be fixed.
   2. But we can re-arrange bit pattern such that instead of `AAAA BBBB CCCC DDDD`, we use `ABCD ABCD ABCD ABCD`
   3. As the result, we only have one bit error in each of section which can be fixed by normal ECC.
5. ADDDC: Adaptive Double DRAM Device Correction
   1. Instead of enabling Lockstep completely, memory controller can move faulty region out of ECC and into virtual lockstep
   2. Depending on SKU, the following might be limited
      1. number of regions
      2. granularity: at bank or at full rank

## non volatile memory

- Data Center Persistent Memory Module (DCPMM)
- NVDIMM-P

RAS Features of the Intel Xeon Scalable Processors on Lenovo ThinkSystem Servers
Super Memory RAS Configuration User's Guide
